[env]
exp_dir = ~/haowen/GitHub/AutoLossApps/
data_dir = ${exp_dir}/Data
model_dir1 = /datasets/BigLearning/haowen/AutoLossApps/saved_models
model_dir2 = /media/haowen/AutoLossApps/saved_models

[data]
train_c_data_file = reg_16_200_4/train_c.npy
valid_c_data_file = reg_16_200_4/valid_c.npy
train_t_data_file = reg_16_200_4/train_t.npy
valid_t_data_file = reg_16_200_4/valid_t.npy
test_data_file = reg_16_200_4/test.npy
num_sample_valid_c = 200
num_sample_train_c = 200
num_sample_valid_t = 200
num_sample_train_t = 200
num_sample_test = 1000
mean_noise = 0
var_noise = 4

[task]
student_model_name =reg 
batch_size_task = 200
dim_input_task = 16
dim_hidden_task = 64
dim_output_task = 1
lr_task = 0.0005
valid_frequency_task = 10
max_endurance_task = 100
max_training_step_task = 10000
lambda1_task = 0.4
lambda2_task = 0.05

[train]

[evaluate]

[meta]
history_len_meta = 2
total_episodes_meta = 400
buffer_size_meta = 200000
ema_decay_state = 0.95
dim_s_meta = 4
dim_h_meta = 16
dim_a_meta = 2
cliprange_meta = 0.2
controller = MlpPPO
batch_size_meta = 512
# adam
lr_meta = 0.001
# sdg
#lr_meta = 0.1
logits_scale_meta = 1
save_frequency_meta = 50
reward_c = 20000
gamma_meta = 0.95
n_parallel_actor = 1
entropy_bonus_beta_meta = 0.001
one_step_td = False
warmup_steps_meta = 20
# Set an max step reward, in case the improve baseline is too small and cause
# huge reward.
reward_max_value = 20
reward_baseline_decay = 0.95
max_endurance_meta = 50
